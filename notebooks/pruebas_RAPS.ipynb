{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e92faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE = 0.95\n",
    "ALPHA = 1-CONFIDENCE\n",
    "LOAD_MODEL_PATH = 'models/AMSC_model02_LAC.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06168e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath('src/'))\n",
    "working_dir = os.getcwd()\n",
    "data_dir = working_dir + '/data/AE_maxillofacial/preprocessed/'\n",
    "\n",
    "#\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "device = 'cuda'\n",
    "\n",
    "# Operaciones aleatorias\n",
    "import random\n",
    "\n",
    "# Manipulación de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Manejo y edición de imágenes\n",
    "from PIL import Image\n",
    "\n",
    "# Evaluación y partición de modelos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modelos, funciones de pérdida y métricas personalizados \n",
    "from conformal_classification_models import *\n",
    "from cp_metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7212ce53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5b20560db0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determina la semilla\n",
    "SEED = 22\n",
    "\n",
    "# Fija la semilla para las operaciones aleatorias en Python puro\n",
    "random.seed(SEED)\n",
    "\n",
    "# Fija la semilla para las operaciones aleatorias en NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Fija la semilla para los generadores aleatorios de PyTorch en CPU\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Fija la semilla para todos los dispositivos GPU (todas las CUDA devices)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Desactiva la autooptimización de algoritmos en cudnn, que puede introducir no determinismo\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fuerza a cudnn a usar operaciones determinísticas (más lento pero reproducible)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Obliga a Pytorch a usar algoritmos determinísticos cuando hay alternativa. Si no la hay, lanza un error.\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Función auxiliar para asegurar que cada worker de DataLoader use una semilla basada en la global\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**SEED\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    " \n",
    "# Generador de números aleatorios para DataLoader\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cebb855",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [transforms.Resize((448, 224)),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomRotation(degrees=3),\n",
    "     transforms.RandomAffine(degrees=0, translate=(0.02, 0.02), scale=(0.95, 1.05)), \n",
    "     transforms.ColorJitter(brightness=0.2, contrast=0.2), \n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.Resize((448, 224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]\n",
    ")\n",
    "\n",
    "class MaxillofacialXRayDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, metadata_file, images_dir, transform=None):\n",
    "        \"\"\"\n",
    "        metadata_file: Ruta al fichero de metadatos (CSV u otro formato)\n",
    "        images_dir: Ruta al directorio de imágenes (entrenamiento o prueba)\n",
    "        transform: Transformaciones a aplicar a las imágenes (normalización, etc.)\n",
    "        \"\"\"\n",
    "        metadata = pd.read_csv(metadata_file)  \n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Preprocesar los campos una sola vez\n",
    "        self.img_paths = metadata['ID'].apply(lambda id_: os.path.join(images_dir, id_)).tolist()\n",
    "        \n",
    "        # Clasificación binaria de sexo: 1 para 'F' y 0 para 'M'\n",
    "        self.sexes = torch.tensor((metadata['Sex'] != 'M').astype(int).values, dtype=torch.long)\n",
    "        \n",
    "        # Edad en float\n",
    "        self.ages = torch.tensor(metadata['Age'].values, dtype=torch.float32)\n",
    "        \n",
    "        # Clasificación binaria de edad: 1 si >= 18, 0 si < 18\n",
    "        self.majority = (self.ages >= 18).long()\n",
    "        \n",
    "        # Etiqueta sigue una clasificación combinada: 0 = (M<18), 1 = (M>=18), 2 = (F<18), 3 = (F>=18)\n",
    "        self.labels = self.sexes * 2 + self.majority\n",
    "        \n",
    "        #\n",
    "        self.num_classes = len(torch.unique(self.labels))\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Carga la imagen\n",
    "        image = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, self.labels[idx]\n",
    "\n",
    "\n",
    "# Crea el Dataset de entrenamiento con augmentations\n",
    "trainset = MaxillofacialXRayDataset(\n",
    "    metadata_file = data_dir + 'metadata_train.csv',\n",
    "    images_dir = data_dir + 'train/',\n",
    "    transform = train_transform\n",
    ")\n",
    "\n",
    "# Crea el Dataset de validación con solo resize y normalización \n",
    "validset = MaxillofacialXRayDataset(\n",
    "    metadata_file = data_dir + 'metadata_train.csv',\n",
    "    images_dir = data_dir + 'train/',\n",
    "    transform = test_transform\n",
    ")\n",
    "\n",
    "# Crea el Dataset de calibración con solo resize y normalización \n",
    "calibset = MaxillofacialXRayDataset(\n",
    "    metadata_file = data_dir + 'metadata_train.csv',\n",
    "    images_dir = data_dir + 'train/',\n",
    "    transform = test_transform\n",
    ")\n",
    "\n",
    "# Crea el Dataset de test con solo resize y normalización\n",
    "testset = MaxillofacialXRayDataset(\n",
    "    metadata_file = data_dir + 'metadata_test.csv',\n",
    "    images_dir = data_dir + 'test/',\n",
    "    transform = test_transform\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c015d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establece un batch size de 32 \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Función optimizada para crear DataLoaders\n",
    "def create_loader(dataset, indices=None, shuffle=False, num_workers=1):\n",
    "    subset = Subset(dataset, indices) if indices is not None else dataset\n",
    "    return DataLoader(\n",
    "        subset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "\n",
    "# Obtiene las edades del trainset por tramos de 0.5 años\n",
    "halfAges = (np.floor(trainset.ages.numpy() * 2) / 2).astype(np.float32)\n",
    "# Hay una única instancia con edad 26, que el algoritmo de separación de entrenamiento y validación será \n",
    "# incapaz de dividir de forma estratificada. Para evitar el error, reasigna esa instancia al tramo \n",
    "# inmediatamente inferior\n",
    "halfAges[halfAges == 26.0] = 25.5\n",
    "\n",
    "# Obtiene el sexo en binario\n",
    "sexes = trainset.sexes.numpy()\n",
    "\n",
    "# Crear etiquetas  combinadas de estratificación (p.ej.: \"18.0_M\", \"17.5_F\")\n",
    "stratify_labels = np.array([f\"{age:.1f}_{sex}\" for age, sex in zip(halfAges, sexes)])\n",
    "\n",
    "# Determina el número de hilos disponibles \n",
    "num_workers = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", 1))\n",
    "\n",
    "# Divide el conjunto de datos completo de entrenamiento en tres subconjuntos de forma estratificada:\n",
    "# - Entrenamiento (68% de las instancias)\n",
    "# - Validación (17% de las instancias)\n",
    "# - Calibración (15% de las instancias)\n",
    "\n",
    "train_idx, calib_idx = train_test_split(range(len(trainset)), train_size=0.8, shuffle=True, \n",
    "                                        random_state=SEED, stratify=stratify_labels)\n",
    "\n",
    "train_idx, valid_idx = train_test_split(train_idx, train_size=0.8, shuffle=True, random_state=SEED,\n",
    "                                        stratify=[stratify_labels[i] for i in train_idx])\n",
    "\n",
    "train_loader = create_loader(trainset, train_idx, shuffle=True, num_workers=num_workers)\n",
    "valid_loader = create_loader(validset, valid_idx)\n",
    "calib_loader = create_loader(calibset, calib_idx)\n",
    "\n",
    "# Crea DataLoader de test\n",
    "test_loader = create_loader(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc1c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instancia el modelo con el nivel de confianza especificado y lo envía a la GPU\n",
    "model = ResNeXtClassifier_RAPS(num_classes=trainset.num_classes, confidence=CONFIDENCE).to(device)\n",
    "\n",
    "# Carga el checkpoint desde el archivo especificado \n",
    "checkpoint = torch.load(LOAD_MODEL_PATH, weights_only=False)\n",
    "\n",
    "# Carga el modelo\n",
    "model.load_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298e60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# model.set_temperature(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171ce5f",
   "metadata": {},
   "source": [
    "### CALIBRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0559bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_reg:  3\n"
     ]
    }
   ],
   "source": [
    "# Pasa por el modelo y obtiene scores y etiquetas verdaderas\n",
    "pred_scores, true_labels = model._inference(valid_loader, return_probs=True)\n",
    "\n",
    "#\n",
    "model.k_reg = model._get_kstar(pred_scores, true_labels)\n",
    "print(\"k_reg: \", model.k_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b84de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "q_hat:  0.9987923502922058\n",
      "Mean size:  2.3755457401275635\n",
      "lambda:  0.001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.lambda_reg = model._get_lambda(pred_scores, true_labels, model.k_reg)\n",
    "print(\"lambda: \", model.lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b899e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.auto_configure(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8f4329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random=False\n",
    "k_reg=1\n",
    "lmbda=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95c563f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997691512107849\n"
     ]
    }
   ],
   "source": [
    "# Obtiene el número de instancias del conjunto y el número de clases\n",
    "n, num_classes = pred_scores.shape\n",
    "\n",
    "# \n",
    "tau = 1-ALPHA\n",
    "\n",
    "# Ordena los scores de mayor a menor y calcula la suma acumulada\n",
    "sorted_scores, sorted_class_perm_index = torch.sort(pred_scores, dim=1, descending=True)\n",
    "cum_sorted_scores = torch.cumsum(sorted_scores, dim=1)\n",
    "\n",
    "# Crea un vector de penalización acumulada para cada posición \n",
    "# (a partir de la posición k_reg en adleante añade penalización cte. de lambda_reg)\n",
    "penalties = torch.zeros((1, num_classes))\n",
    "penalties[0, k_reg:] += lmbda\n",
    "cumulative_penalties = torch.cumsum(penalties, dim=1)\n",
    "\n",
    "# Encuentra, para cada instancia, la posición (0-indexed) de la clase verdadera en el ranking\n",
    "matches = (sorted_class_perm_index == true_labels.unsqueeze(1))\n",
    "true_class_rank = matches.int().argmax(dim=1)  # (n,)\n",
    "\n",
    "# Recolecta los scores y suma acumulada en la posición del índice verdadero\n",
    "true_score = sorted_scores[torch.arange(n), true_class_rank]\n",
    "true_cum_score = cum_sorted_scores[torch.arange(n), true_class_rank]\n",
    "\n",
    "# Obtiene las penalizaciones y suma acumulada en la posición del índice verdadero\n",
    "true_penalty = penalties[0, true_class_rank]\n",
    "true_cum_penalty = cumulative_penalties[0, true_class_rank]\n",
    "\n",
    "if random:\n",
    "    \n",
    "    # Genera un vector de valores aleatorios entre 0 y 1, uno por instancia \n",
    "    U = torch.rand(n, generator=g)\n",
    "    \n",
    "    #\n",
    "    nonconformity_scores = torch.where(\n",
    "        true_class_rank >= 1,\n",
    "        true_cum_score + true_cum_penalty - U * true_score,\n",
    "        true_cum_score + true_cum_penalty\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    nonconformity_scores = true_cum_score + true_cum_penalty\n",
    "\n",
    "\n",
    "# Calcula el nivel de cuantificación ajustado basado en el tamaño del conjunto de calibración y alpha\n",
    "q_level = math.ceil((1.0 - ALPHA) * (n + 1.0)) / n\n",
    "\n",
    "# Calcula el umbral de no conformidad como el percentil q_level de los valores de conformidad \n",
    "q_hat = torch.quantile(nonconformity_scores, q_level, interpolation='higher').item()\n",
    "\n",
    "print(q_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e2874",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c89a2eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene las predicciones y las clases verdaderas para el conjunto de evaluación\n",
    "test_pred_scores, test_true_labels = model._inference(test_loader, return_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fab9f8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 0, 1, 1],\n",
       "        ...,\n",
       "        [0, 1, 1, 1],\n",
       "        [1, 0, 1, 1],\n",
       "        [0, 0, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determina la clase predicha como la clase con mayor puntuación\n",
    "_, test_pred_labels = torch.max(test_pred_scores, dim=1) \n",
    "\n",
    "# Obtiene el número de instancias del conjunto y el número de clases\n",
    "n, num_classes = test_pred_scores.shape\n",
    "\n",
    "# Ordena los scores de mayor a menor y calcula la suma acumulada\n",
    "sorted_scores, sorted_class_perm_index = torch.sort(test_pred_scores, dim=1, descending=True)\n",
    "cum_sorted_scores = torch.cumsum(sorted_scores, dim=1)\n",
    "\n",
    "#\n",
    "penalties = torch.zeros((1, num_classes))\n",
    "penalties[0, k_reg:] += lmbda\n",
    "cumulative_penalties = torch.cumsum(penalties, dim=1)\n",
    "\n",
    "#\n",
    "matches = cum_sorted_scores <= q_hat\n",
    "has_match = matches.any(dim=1)\n",
    "last_class_rank = torch.where(\n",
    "    has_match,\n",
    "    matches.int().sum(dim=1)-1,\n",
    "    torch.ones(n, dtype=torch.uint8)\n",
    ")\n",
    "\n",
    "#\n",
    "if random:\n",
    "    # Genera un vector de valores aleatorios entre 0 y 1, uno por instancia \n",
    "    U = torch.rand(n)\n",
    "    \n",
    "    #\n",
    "    last_score = sorted_scores[torch.arange(n), last_class_rank]\n",
    "    last_cum_scores = cum_sorted_scores[torch.arange(n), last_class_rank]\n",
    "    \n",
    "    #\n",
    "    last_penalty = penalties[0, last_class_rank]\n",
    "    last_cum_penalty = cumulative_penalties[0, last_class_rank]\n",
    "    \n",
    "    #\n",
    "    V = (last_cum_scores + last_cum_penalty - q_hat) / (last_score + last_penalty)\n",
    "    \n",
    "    #\n",
    "    last_class_ranks = torch.where(\n",
    "        (U <= V) & (last_class_rank>=1),\n",
    "        last_class_rank-1,\n",
    "        last_class_rank\n",
    "    )\n",
    "\n",
    "#\n",
    "idx = torch.arange(num_classes)\n",
    "inclusion_mask = idx <= last_class_rank.unsqueeze(1)\n",
    "\n",
    "# \n",
    "test_pred_sets = torch.zeros_like(test_pred_scores, dtype=torch.uint8)\n",
    "test_pred_sets.scatter_(1, sorted_class_perm_index, inclusion_mask.to(torch.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0ea2abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.79 %\n",
      "Cobertura empírica: 96.84 %\n",
      "Tamaño medio de conjunto: 2.66\n"
     ]
    }
   ],
   "source": [
    "# Calcula y muestra la exactitud\n",
    "accry = (test_pred_labels == test_true_labels).sum() / test_true_labels.size(0)\n",
    "print(f\"Accuracy: {accry*100:>4.2f} %\")\n",
    "\n",
    "# Calcula y muestra la cobertura empírica \n",
    "ec = empirical_coverage_classification(test_pred_sets, test_true_labels)\n",
    "print(f\"Cobertura empírica: {ec*100:>4.2f} %\")\n",
    "\n",
    "# Calcula y muestra el tamaño medio del conjunto\n",
    "mss = mean_set_size(test_pred_sets)\n",
    "print(f\"Tamaño medio de conjunto: {mss:>4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824477d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
