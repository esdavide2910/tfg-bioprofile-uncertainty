\chapter{Métricas en problemas de clasificación}

Una vez definido los tipo de problemas de clasificación, es fundamental establecer cómo medir la efectividad 
del modelo predictivo. A continuación, se detallan los principales criterios y elementos gráficos utilizados 
para evaluar y comparar modelos de clasificación:

\begin{itemize}
    
    \item La \textbf{matriz de confusión} es una herramienta fundamental que permite visualizar el rendimiento 
    de modelos de clasificación, tanto binarios como multiclase. Esta muestra una tabla con tantas columnas y 
    filas como clases haya. En un eje, se representan las clases reales (etiquetas verdaderas), y en el otro 
    eje, las clases predichas por el modelo. Cada celda de la matriz indica la cantidad de ejemplos que 
    pertenecen a una clase real específica y que han sido clasificados como una clase predicha específica 
    (véase la Figura \ref{fig:conf_matrix_binary}).

    Idealmente, los valores se concentrarían en la diagonal principal, lo que indicaría que las predicciones 
    coinciden con los valores reales.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\textwidth]{capitulos/cap_02/imagenes/confusion_matrix_binary.png}
        \caption{
            Matriz de confusión para la estimación de sexo según el modelo \textit{random forest} 
            propuesto en \cite{bidmos2023}.
        } 
        \label{fig:conf_matrix_binary}
    \end{figure}

    Esta visualización admite muchas variantes, por ejemplo, como la vista en la Figura 
    \ref{fig:conf_matrix_binary_relative}.
    
    \begin{figure}[h]
        \centering
    
        \begin{subfigure}[b]{0.3\textwidth}
            \centering
            \includegraphics[width=\textwidth]{capitulos/cap_02/imagenes/confusion_matrix_binary_1.png}
            \caption{Sin información de sexo}
            \label{fig:conf_matrix_general}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.3\textwidth}
            \centering
            \includegraphics[width=\textwidth]{capitulos/cap_02/imagenes/confusion_matrix_binary_2.png}
            \caption{Sexo femenino}
            \label{fig:conf_matrix_female}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.3\textwidth}
            \centering
            \includegraphics[width=\textwidth]{capitulos/cap_02/imagenes/confusion_matrix_binary_3.png}
            \caption{Sexo masculino}
            \label{fig:conf_matrix_male}
        \end{subfigure}
    
        \caption[
            Matrices de confusión para la estimación de mayoría/minoría de edad según el modelo de 
            \cite{porto2020}.
        ]{
            Matrices de confusión para la estimación de mayoría/minoría de edad según el modelo de 
            \cite{porto2020}.
            Se representan los valores de cada celda en términos porcentuales de los ejemplos reales que hay 
            de cada clase ($< 18$ y $\ge 18$), lo que permite comparar la matriz de confusión general de todos 
            los ejemplos (\ref{sub@fig:conf_matrix_general}) con la de ejemplos se sexo femenino 
            (\ref{sub@fig:conf_matrix_female}) y sexo masculino (\ref{sub@fig:conf_matrix_male}), permitiendo 
            identificar posibles sesgos en el modelo respecto al género, y así realizar una evaluación más 
            precisa del rendimiento del modelo en diferentes subgrupos de la población.
        }
        \label{fig:conf_matrix_binary_relative}
    \end{figure}

    Prácticamente todas las métricas y visualizaciones parten de la información ofrecida en esta matriz. 
    

    \item La \textbf{exactitud (\textit{accuracy})} es la proporción de predicciones correctas sobre el total.
    En clasificación binaria esto sería:
    
    $$
    \textnormal{Accuracy} = \frac{TP+TN}{TP+TN+FP+FN}
    $$

    En el caso de clasificación multiclase, esta se generaliza como:

    $$
    \textnormal{Accuracy} = 
        \frac{\textnormal{Numero de predicciones correctas}}{\textnormal{Número total de ejemplos}} =
        \frac{1}{N} \sum_{i=1}^N1(\hat{y_i}=y_i)
    $$

    donde $\hat{y_i}$ es la etiqueta predicha, $y_i$ la etiqueta verdadera para el ejemplo $i$, $N$ es el 
    número de ejemplos, y 1 es la función indicadora, que vale 1 si la predicción es correcta y 0 si no 
    lo es. 

    Los valores de esta métrica varían entre 0 y 1, donde 0 es el peor desempeño posible (todas las 
    predicciones son incorrectas) y 1 es el mejor desempeño posible (todas las predicciones son correctas).

    Es la medida más intuitiva, si bien puede dar una falsa impresión de buen desempeño si las clases 
    mayoritarias dominan la métrica. Es por esto que el análisis debe completarse con otras métricas 
    informativas. 


    \item La \textbf{precisión (\textit{precision})} indica qué proporción de las predicciones positivas 
    corresponde a casos realmente positivos. En clasificación multiclase, se interpreta como la proporción de 
    ejemplos correctamente clasificados entre todos los que fueron asignados a una clase determinada. 

    $$
    \textnormal{Precision} = \frac{TP}{TP+FP}
    $$

    Una alta precisión significa pocos falsos positivos. Esto puede interesar por ejemplo

    La \textbf{exhaustividad (\textit{recall})} indica qué proporción de los casos positivos fueron 
    correctamente detectados.

    $$
    \textnormal{Recall} = \frac{TP}{TP+FN} 
    $$

    Un alto \textit{recall} significa pocos falsos negativos. 

    
    Estas dos métricas complementarias se pueden calcular por cada clase, y hay varias formas de combinar sus
    valores:

    \begin{itemize}
        \item Macro:
        \item Micro:
        \item Weighted: 
    \end{itemize}
    

    \item El \textbf{F1-Score} 
    \todo{Por completar}

    $$
    \textnormal{F1-Score} = 2 \cdot \frac{\textnormal{Precision} \cdot \textnormal{Recall}}{\textnormal{Precision} + \textnormal{Recall}}
    $$

    Al igual que con \textit{precision} y \textit{recall}, también se puede calcular por cada clase. 


\end{itemize}